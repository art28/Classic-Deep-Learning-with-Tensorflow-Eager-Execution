{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir, remove\n",
    "from os.path import isfile, join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/aksdmj/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide filenames by its sentiment\n",
    "# error-avoiding method\n",
    "documents = defaultdict(list)\n",
    "for i in mr.fileids():\n",
    "    documents[i.split('/')[0]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'neg'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(documents['pos']), len(documents['neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate maximum length of text\n",
    "lens = [len(mr.words(i)) for i  in mr.fileids()]\n",
    "max_num_word = max(lens)\n",
    "max_num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique words used in all texts\n",
    "unique_words = len(set(mr.words()))\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map from index to word\n",
    "vocab_dict = {i:v for i,v in enumerate(set(mr.words()))}\n",
    "vocab_dict[10495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10495"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map from word to index\n",
    "vocab_dict_inv = {v:i for i,v in vocab_dict.items()}\n",
    "vocab_dict_inv['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already have pretrained wordvector weights, replace weightpath variable with a path of it\n",
    "weight_dir = os.path.join(os.path.abspath('..'),\"word2vec\")\n",
    "weight_path = os.path.join(weight_dir, \"GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory is already exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "else:\n",
    "    print(\"Directory is already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained weight is already exist\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(weight_path):\n",
    "    print(\"No pretrained weight file, start download...\")\n",
    "    gdd.download_file_from_google_drive(file_id='0B7XkCwpI5KDYNlNUTTlSS21pQmM',\n",
    "                                        dest_path=weight_path + '.gz',\n",
    "                                        unzip=False)\n",
    "    inF = gzip.open(weight_path + '.gz', 'rb')\n",
    "    outF = open(weight_path, 'wb')\n",
    "    outF.write(inF.read())\n",
    "    inF.close()\n",
    "    outF.close()\n",
    "\n",
    "    remove(w2v_path + '.gz')\n",
    "\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"pretrained weight is already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained vector\n",
    "w2v = KeyedVectors.load_word2vec_format(weight_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39768, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make w2v matrix for our dataset's words\n",
    "weights = np.array([w2v[v] if v in w2v else np.zeros(w2v.vector_size) for i ,v in vocab_dict.items()])\n",
    "# (number of words, dimension of wordvectors)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2indexs(words):\n",
    "    return np.array([vocab_dict_inv[word] for word in words])\n",
    "\n",
    "# make index array with size of (max_num_word), with -1(\"UNK\" word) padding\n",
    "def preprocess(document):\n",
    "    indexs = words2indexs(mr.words(document))\n",
    "    return np.concatenate([indexs, -1*np.ones([max_num_word - indexs.shape[0]], dtype=\"int64\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "y = list()\n",
    "\n",
    "# 0 label for negative , 1 for positive\n",
    "for i in documents['neg']:\n",
    "    tx = preprocess(i)\n",
    "    X.append(tx)\n",
    "    y.append(0)\n",
    "    \n",
    "for i in documents['pos']:\n",
    "    tx = preprocess(i)\n",
    "    X.append(tx)\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split with the ratio of (0.72, 0.18, 0.1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "data_val = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(len(y_val))\n",
    "data_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Sentence(tf.keras.Model):\n",
    "    \"\"\" Classifier for Movie Review dataset\n",
    "    Args:\n",
    "        num_words: uniwue words in dataset. including the words that is not in pretrained w2v model.\n",
    "        in_dim: dimension of input array, which is maximum word counts among texts.\n",
    "        w2v_dim: word2vectors representationo dimension. 300 for GoogleNews-vectors-negative300 weights.\n",
    "        out_dim: softmax output dimension.\n",
    "        is_static: if True, use static(no change in w2v pretrained weights), otherwise modify weights.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_words=unique_words,\n",
    "                 in_dim= max_num_word,\n",
    "                 w2v_dim= 300,\n",
    "                 out_dim=2,\n",
    "                 learning_rate=0.001,\n",
    "                 is_static=True,\n",
    "                 checkpoint_directory=\"checkpoints/\",\n",
    "                 device_name=\"cpu:0\"):\n",
    "        super(CNN_Sentence, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.num_words = num_words\n",
    "        self.out_dim = out_dim\n",
    "        self.is_static = is_static\n",
    "        self.learning_rate = learning_rate\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.device_name = device_name\n",
    "\n",
    "        self.w2v = tf.keras.layers.Embedding(num_words,w2v_dim)\n",
    "\n",
    "        self.conv11 = tf.layers.Conv1D(filters=100, kernel_size=3, padding=\"valid\")\n",
    "        self.conv12 = tf.layers.Conv1D(100, 4, padding=\"valid\")\n",
    "        self.conv13 = tf.layers.Conv1D(100, 5, padding=\"valid\")\n",
    "\n",
    "        self.maxpool1 = tf.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        self.maxpool2 = tf.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        self.maxpool3 = tf.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "\n",
    "        self.flatten = tf.layers.Flatten()\n",
    "        self.dropout = tf.layers.Dropout(0.5)\n",
    "\n",
    "        self.out = tf.layers.Dense(self.out_dim)        \n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # global step\n",
    "        self.global_step = 0\n",
    "        \n",
    "        # verbose logging\n",
    "        self.epoch_loss = 0\n",
    "        \n",
    "        # plotting\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_accuracies = []\n",
    "        \n",
    "\n",
    "    def copy_pretrained(self, weights):\n",
    "        # init\n",
    "        self.predict(tf.convert_to_tensor(np.zeros([1, self.num_words]), dtype=\"float64\"), False)\n",
    "        temp = tf.convert_to_tensor(weights.astype(\"float32\"))\n",
    "        tf.assign(self.w2v.weights[0], temp)\n",
    "\n",
    "    def predict(self, X, training):\n",
    "        \n",
    "        X = self.w2v(X)\n",
    "\n",
    "        x11 = self.conv11(X)\n",
    "        x12 = self.conv12(X)\n",
    "        x13 = self.conv13(X)\n",
    "\n",
    "        x11m = tf.reduce_max(x11,1)\n",
    "        x12m = tf.reduce_max(x12,1)\n",
    "        x13m = tf.reduce_max(x13,1)\n",
    "\n",
    "        xf1 = self.flatten(x11m)\n",
    "        xf2 = self.flatten(x12m)\n",
    "        xf3 = self.flatten(x13m)\n",
    "\n",
    "        xf = tf.concat([xf1, xf2, xf3], axis=1)\n",
    "\n",
    "        if training:\n",
    "            xf = self.dropout(xf)\n",
    "\n",
    "        pred = self.out(xf)\n",
    "        return pred\n",
    "\n",
    "    def loss(self, X, y, training):\n",
    "        prediction = self.predict(X, training)\n",
    "        loss_val = tf.losses.sparse_softmax_cross_entropy(y, prediction)\n",
    "        loss_val += tf.nn.l2_loss(self.out.weights[0])\n",
    "        self.epoch_loss += loss_val.numpy()\n",
    "        \n",
    "        return loss_val\n",
    "\n",
    "    def grad(self, X, y, training):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss_val = self.loss(X, y, training)\n",
    "        return tape.gradient(loss_val, self.variables) \n",
    "        \n",
    "    def fit(self, train_data, validation_data, test_data, epochs=1, verbose=1, \n",
    "            batch_size=50, saving=False, early_stopping=0):\n",
    "    \n",
    "        train_data_batch = train_data.shuffle(100).batch(batch_size)\n",
    "        \n",
    "        with tf.device(self.device_name):\n",
    "            for i in range(epochs):\n",
    "                self.epoch_loss = 0\n",
    "                for X, y in tfe.Iterator(train_data_batch):\n",
    "                    grads = self.grad(X, y, True)\n",
    "                    if self.is_static:\n",
    "                        self.optimizer.apply_gradients(zip(grads[1:], self.variables[1:]))\n",
    "                    else :\n",
    "                        self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                self.global_step += 1\n",
    "                \n",
    "                self.train_losses.append(self.epoch_loss)\n",
    "                \n",
    "                for X, y in tfe.Iterator(validation_data):\n",
    "                    self.val_losses.append(self.loss(X, y, False).numpy())\n",
    "                    \n",
    "                    \n",
    "                accuracy = tfe.metrics.Accuracy('train_acc')\n",
    "\n",
    "                for X, y in tfe.Iterator(test_data):\n",
    "                    logits = self.predict(X=X, training=False)\n",
    "                    predictions = tf.argmax(logits, axis=1, output_type=\"int32\")\n",
    "                    accuracy(predictions, y)     \n",
    "                    \n",
    "                self.test_accuracies.append(accuracy.result().numpy())\n",
    "\n",
    "                if (i+1)%verbose == 0 :\n",
    "                    print(\"[EPOCH %d / STEP %d]\" % ((i + 1), self.global_step))\n",
    "                    print(\"Train loss : %s\" % self.train_losses[-1])\n",
    "                    print(\"Val   loss : %s\" % self.val_losses[-1])\n",
    "                    print(\"TEST accuracy: %.4f%%\" % (self.test_accuracies[-1] * 100))\n",
    "                    print()\n",
    "                          \n",
    "\n",
    "                accuracy.init_variables()\n",
    "                \n",
    "                if early_stopping:\n",
    "                    ok = False\n",
    "                    if len(self.val_losses) <= early_stopping:\n",
    "                        continue\n",
    "                    for i in range(early_stopping):\n",
    "                        if self.val_losses[-(i+1)] < self.val_losses[-(i+2)]:\n",
    "                            ok = True\n",
    "                            break\n",
    "                    if ok:\n",
    "                        continue\n",
    "                    print(\"early stopping on step %s\" % self.global_step)\n",
    "                    break\n",
    "                                              \n",
    "                    \n",
    "                    \n",
    "    def save(self):\n",
    "        tfe.Saver(self.variables).save(self.checkpoint_directory, global_step=self.global_step)\n",
    "\n",
    "    def load(self, global_step=\"latest\"):\n",
    "        # init\n",
    "        self.predict(tf.convert_to_tensor(np.zeros([1, self.num_words]), dtype=\"float64\"), False)\n",
    "\n",
    "        saver = tfe.Saver(self.variables)\n",
    "        if global_step == \"latest\":\n",
    "            saver.restore(tf.train.latest_checkpoint(self.checkpoint_directory))\n",
    "            self.global_step = int(tf.train.latest_checkpoint(self.checkpoint_directory).split('/')[-1][1:])\n",
    "        else:\n",
    "            saver.restore(self.checkpoint_directory + \"-\" + str(global_step))\n",
    "            self.global_step = global_step\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_static = CNN_Sentence(device_name=\"gpu:0\")\n",
    "# if you don't have tensorflow-gpu, then erase device_name parameter\n",
    "# cnn_static = CNN_Sentence()\n",
    "cnn_static.copy_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2 / STEP 2]\n",
      "Train loss : 38.306246876716614\n",
      "Val   loss : 1.1973715\n",
      "TEST accuracy: 66.0000%\n",
      "\n",
      "[EPOCH 4 / STEP 4]\n",
      "Train loss : 20.28722643852234\n",
      "Val   loss : 0.771232\n",
      "TEST accuracy: 75.0000%\n",
      "\n",
      "[EPOCH 6 / STEP 6]\n",
      "Train loss : 14.574689149856567\n",
      "Val   loss : 0.6395987\n",
      "TEST accuracy: 81.0000%\n",
      "\n",
      "[EPOCH 8 / STEP 8]\n",
      "Train loss : 12.121951282024384\n",
      "Val   loss : 0.58996224\n",
      "TEST accuracy: 82.0000%\n",
      "\n",
      "[EPOCH 10 / STEP 10]\n",
      "Train loss : 10.397034466266632\n",
      "Val   loss : 0.56847554\n",
      "TEST accuracy: 83.5000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_static.fit(data_train, data_val,  data_test, epochs=10, verbose=2, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_non_static = CNN_Sentence(device_name=\"gpu:0\", is_static=False)\n",
    "cnn_non_static.copy_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2 / STEP 2]\n",
      "Train loss : 37.34070706367493\n",
      "Val   loss : 1.2378596\n",
      "TEST accuracy: 71.0000%\n",
      "\n",
      "[EPOCH 4 / STEP 4]\n",
      "Train loss : 17.38844782114029\n",
      "Val   loss : 0.76585174\n",
      "TEST accuracy: 83.0000%\n",
      "\n",
      "[EPOCH 6 / STEP 6]\n",
      "Train loss : 10.876617908477783\n",
      "Val   loss : 0.6033069\n",
      "TEST accuracy: 84.0000%\n",
      "\n",
      "[EPOCH 8 / STEP 8]\n",
      "Train loss : 7.60062512755394\n",
      "Val   loss : 0.53781915\n",
      "TEST accuracy: 82.0000%\n",
      "\n",
      "[EPOCH 10 / STEP 10]\n",
      "Train loss : 5.6968440264463425\n",
      "Val   loss : 0.50722325\n",
      "TEST accuracy: 83.0000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_non_static.fit(data_train, data_val,  data_test, epochs=10, verbose=2, early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: regression on Amazon ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"amazon_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get dataset in \n",
    "# http://jmcauley.ucsd.edu/data/amazon/\n",
    "df = getDF('../amazon_reviews/reviews_Beauty_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>Andrea</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>Very oily and creamy. Not at all what I expect...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>1391040000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A1YJEY40YUW4SE</td>\n",
       "      <td>7806397051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK Palette!</td>\n",
       "      <td>Jessica H.</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This palette was a decent price and I was look...</td>\n",
       "      <td>04 18, 2014</td>\n",
       "      <td>1397779200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A60XNB876KYML</td>\n",
       "      <td>7806397051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great quality</td>\n",
       "      <td>Karen</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>The texture of this concealer pallet is fantas...</td>\n",
       "      <td>09 6, 2013</td>\n",
       "      <td>1378425600</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A3G6XNM240RMWA</td>\n",
       "      <td>7806397051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do not work on my face</td>\n",
       "      <td>Norah</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I really can't tell what exactly this thing is...</td>\n",
       "      <td>12 8, 2013</td>\n",
       "      <td>1386460800</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A1PQFP6SAJ6D80</td>\n",
       "      <td>7806397051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's okay.</td>\n",
       "      <td>Nova Amor</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>It was a little smaller than I expected, but t...</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A38FVHZTNQ271F</td>\n",
       "      <td>7806397051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  summary reviewerName helpful  \\\n",
       "0  Don't waste your money       Andrea  [3, 4]   \n",
       "1             OK Palette!   Jessica H.  [1, 1]   \n",
       "2           great quality        Karen  [0, 1]   \n",
       "3  Do not work on my face        Norah  [2, 2]   \n",
       "4              It's okay.    Nova Amor  [0, 0]   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Very oily and creamy. Not at all what I expect...  01 30, 2014   \n",
       "1  This palette was a decent price and I was look...  04 18, 2014   \n",
       "2  The texture of this concealer pallet is fantas...   09 6, 2013   \n",
       "3  I really can't tell what exactly this thing is...   12 8, 2013   \n",
       "4  It was a little smaller than I expected, but t...  10 19, 2013   \n",
       "\n",
       "   unixReviewTime  overall      reviewerID        asin  \n",
       "0      1391040000      1.0  A1YJEY40YUW4SE  7806397051  \n",
       "1      1397779200      3.0   A60XNB876KYML  7806397051  \n",
       "2      1378425600      4.0  A3G6XNM240RMWA  7806397051  \n",
       "3      1386460800      2.0  A1PQFP6SAJ6D80  7806397051  \n",
       "4      1382140800      3.0  A38FVHZTNQ271F  7806397051  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2wordcounts(text):\n",
    "    words = text.lower().split()\n",
    "    return np.array(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counts'] = df['reviewText'].apply(text2wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>Andrea</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>Very oily and creamy. Not at all what I expect...</td>\n",
       "      <td>01 30, 2014</td>\n",
       "      <td>1391040000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A1YJEY40YUW4SE</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK Palette!</td>\n",
       "      <td>Jessica H.</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This palette was a decent price and I was look...</td>\n",
       "      <td>04 18, 2014</td>\n",
       "      <td>1397779200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A60XNB876KYML</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great quality</td>\n",
       "      <td>Karen</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>The texture of this concealer pallet is fantas...</td>\n",
       "      <td>09 6, 2013</td>\n",
       "      <td>1378425600</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A3G6XNM240RMWA</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do not work on my face</td>\n",
       "      <td>Norah</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I really can't tell what exactly this thing is...</td>\n",
       "      <td>12 8, 2013</td>\n",
       "      <td>1386460800</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A1PQFP6SAJ6D80</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's okay.</td>\n",
       "      <td>Nova Amor</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>It was a little smaller than I expected, but t...</td>\n",
       "      <td>10 19, 2013</td>\n",
       "      <td>1382140800</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A38FVHZTNQ271F</td>\n",
       "      <td>7806397051</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  summary reviewerName helpful  \\\n",
       "0  Don't waste your money       Andrea  [3, 4]   \n",
       "1             OK Palette!   Jessica H.  [1, 1]   \n",
       "2           great quality        Karen  [0, 1]   \n",
       "3  Do not work on my face        Norah  [2, 2]   \n",
       "4              It's okay.    Nova Amor  [0, 0]   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Very oily and creamy. Not at all what I expect...  01 30, 2014   \n",
       "1  This palette was a decent price and I was look...  04 18, 2014   \n",
       "2  The texture of this concealer pallet is fantas...   09 6, 2013   \n",
       "3  I really can't tell what exactly this thing is...   12 8, 2013   \n",
       "4  It was a little smaller than I expected, but t...  10 19, 2013   \n",
       "\n",
       "   unixReviewTime  overall      reviewerID        asin  counts  \n",
       "0      1391040000      1.0  A1YJEY40YUW4SE  7806397051      28  \n",
       "1      1397779200      3.0   A60XNB876KYML  7806397051      27  \n",
       "2      1378425600      4.0  A3G6XNM240RMWA  7806397051     102  \n",
       "3      1386460800      2.0  A1PQFP6SAJ6D80  7806397051      35  \n",
       "4      1382140800      3.0  A38FVHZTNQ271F  7806397051      65  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306 rows among 198502 rows have length more than 500\n"
     ]
    }
   ],
   "source": [
    "wordlen = 500\n",
    "print(\"%s rows among %s rows have length more than %s\" % (df[df['counts']>wordlen].count()[0], len(df), wordlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2words(text):\n",
    "    words = np.array(text.lower().split())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['very', 'oily', 'and', 'creamy.', 'not', 'at', 'all', 'what', 'i',\n",
       "       'expected...', 'ordered', 'this', 'to', 'try', 'to', 'highlight',\n",
       "       'and', 'contour', 'and', 'it', 'just', 'looked', 'awful!!!',\n",
       "       'plus,', 'took', 'forever', 'to', 'arrive.'], dtype='<U11')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['reviewText'].apply(text2words).values\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply normalization on score from 1 ~ 5 to 0.2 ~ 1.0\n",
    "y = np.array(df['overall'].values)/5.0\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning, it tooks more than 100GB for all preprocessed data.\n",
    "# recommand to use minimized version using code below\n",
    "# X = X[:2000]\n",
    "# y = y[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Regression(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 in_dim= wordlen,\n",
    "                 w2v_dim= 300,\n",
    "                 out_dim=1,\n",
    "                 learning_rate=0.001,\n",
    "                 checkpoint_directory=\"checkpoints/\",\n",
    "                 device_name=\"cpu:0\"):\n",
    "        super(CNN_Regression, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.w2v_dim = w2v_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self. checkpoint_directory = checkpoint_directory\n",
    "        self.device_name = device_name\n",
    "\n",
    "        self.conv11 = tf.layers.Conv1D(filters=100, kernel_size=3, padding=\"valid\")\n",
    "        self.conv12 = tf.layers.Conv1D(100, 4, padding=\"valid\")\n",
    "        self.conv13 = tf.layers.Conv1D(100, 5, padding=\"valid\")\n",
    "\n",
    "        self.maxpool1 = tf.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        self.maxpool2 = tf.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        self.maxpool3 = tf.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "\n",
    "        self.flatten = tf.layers.Flatten()\n",
    "        self.dropout = tf.layers.Dropout(0.5)\n",
    "\n",
    "        self.out = tf.layers.Dense(self.out_dim, activation=tf.nn.sigmoid)        \n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # global step\n",
    "        self.global_step = 0\n",
    "        \n",
    "        # verbose logging\n",
    "        self.epoch_loss = 0\n",
    "        \n",
    "        # plotting\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "        \n",
    "    def preprocess(self, X):\n",
    "        x = np.zeros((len(X), self.in_dim, self.w2v_dim), dtype=\"float32\")\n",
    "        for i, batch in enumerate(X):\n",
    "            for j, word in enumerate(batch):\n",
    "                if j >= 500:\n",
    "                    break\n",
    "                if word in w2v:\n",
    "                    x[i][j] = w2v[word]\n",
    "        return tf.convert_to_tensor(x)\n",
    "        \n",
    "    def predict(self, X, training):\n",
    "        \n",
    "        x11 = self.conv11(X)\n",
    "        x12 = self.conv12(X)\n",
    "        x13 = self.conv13(X)\n",
    "\n",
    "        x11m = tf.reduce_max(x11,1)\n",
    "        x12m = tf.reduce_max(x12,1)\n",
    "        x13m = tf.reduce_max(x13,1)\n",
    "\n",
    "        xf1 = self.flatten(x11m)\n",
    "        xf2 = self.flatten(x12m)\n",
    "        xf3 = self.flatten(x13m)\n",
    "\n",
    "        xf = tf.concat([xf1, xf2, xf3], axis=1)\n",
    "\n",
    "        if training:\n",
    "            xf = self.dropout(xf)\n",
    "\n",
    "        pred = self.out(xf)\n",
    "        return pred\n",
    "\n",
    "    def loss(self, X, y, training):\n",
    "        prediction = self.predict(X, training)\n",
    "        loss_val = tf.losses.mean_squared_error(y, prediction)\n",
    "#         loss_val += tf.nn.l2_loss(self.out.weights[0])\n",
    "        self.epoch_loss += loss_val.numpy()\n",
    "        \n",
    "        return loss_val\n",
    "\n",
    "    def grad(self, X, y, training):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss_val = self.loss(X, y, training)\n",
    "        return tape.gradient(loss_val, self.variables) \n",
    "        \n",
    "    def batch_preprocess(self, X_train, y_train, batch_size):\n",
    "        from datetime import datetime\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        batch_dir = os.path.abspath(\"/tmp/%s\" % timestamp)\n",
    "        os.makedirs(batch_dir)\n",
    "        for i in range(len(X_train)//batch_size + 1):\n",
    "            X = X_train[50*i:min(50*(i+1), len(X_train))]\n",
    "            X = self.preprocess(X)\n",
    "            y = y_train[50*i:min(50*(i+1), len(y_train))].reshape(-1,1)\n",
    "            filenameX = os.path.join(batch_dir, \"batchX_%s.npy\" % i)\n",
    "            filenamey = os.path.join(batch_dir, \"batchy_%s.npy\" % i)\n",
    "            np.save(filenameX, X)\n",
    "            np.save(filenamey, y)\n",
    "        \n",
    "        return batch_dir\n",
    "            \n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val, X_test, y_test, epochs=1, verbose=1, \n",
    "            batch_size=50, saving=False, early_stopping=0):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        print(\"preprocessing...  \", end=\"\")\n",
    "        batch_dir = self.batch_preprocess(X_train, y_train, batch_size)\n",
    "        print(\"Done %s second\" % (time.time()-start))\n",
    "        \n",
    "        X_val = self.preprocess(X_val)\n",
    "        X_test = self.preprocess(X_test)\n",
    "        \n",
    "        with tf.device(self.device_name):\n",
    "            for i in range(epochs):\n",
    "                self.epoch_loss = 0\n",
    "                for j in range(len(X_train)//batch_size + 1):\n",
    "                    filenameX = os.path.join(batch_dir, \"batchX_%s.npy\" % j)\n",
    "                    filenamey = os.path.join(batch_dir, \"batchy_%s.npy\" % j)\n",
    "                    X = tf.convert_to_tensor(np.load(filenameX))\n",
    "                    y = tf.convert_to_tensor(np.load(filenamey))\n",
    "                    \n",
    "                    grads = self.grad(X, y, True)\n",
    "\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                self.global_step += 1\n",
    "                \n",
    "                self.train_losses.append(self.epoch_loss)\n",
    "                \n",
    "                self.val_losses.append(self.loss(X_val, y_val.reshape(-1,1), False).numpy())\n",
    "                    \n",
    "                self.test_losses.append(self.loss(X_test, y_test.reshape(-1,1), False).numpy())\n",
    "                \n",
    "\n",
    "                if (i+1)%verbose == 0 :\n",
    "                    print(\"[EPOCH %d / STEP %d]\" % ((i + 1), self.global_step))\n",
    "                    print(\"Train loss : %s\" % self.train_losses[-1])\n",
    "                    print(\"Val   loss : %s\" % self.val_losses[-1])\n",
    "                    print(\"Test  loss : %.4f\" % (self.test_losses[-1]))\n",
    "                    print()\n",
    "                          \n",
    "\n",
    "                \n",
    "                if early_stopping:\n",
    "                    ok = False\n",
    "                    if len(self.val_losses) <= early_stopping:\n",
    "                        continue\n",
    "                    for i in range(early_stopping):\n",
    "                        if self.val_losses[-(i+1)] < self.val_losses[-(i+2)]:\n",
    "                            ok = True\n",
    "                            break\n",
    "                    if ok:\n",
    "                        continue\n",
    "                    print(\"early stopping on step %s\" % self.global_step)\n",
    "                    break\n",
    "                                              \n",
    "                    \n",
    "                    \n",
    "    def save(self):\n",
    "        tfe.Saver(self.variables).save(self.checkpoint_directory, global_step=self.global_step)\n",
    "\n",
    "    def load(self, global_step=\"latest\"):\n",
    "        # init\n",
    "        self.predict(tf.convert_to_tensor(np.zeros([1, self.in_dim]), dtype=\"float32\"), False)\n",
    "\n",
    "        saver = tfe.Saver(self.variables)\n",
    "        if global_step == \"latest\":\n",
    "            saver.restore(tf.train.latest_checkpoint(self.checkpoint_directory))\n",
    "            self.global_step = int(tf.train.latest_checkpoint(self.checkpoint_directory).split('/')[-1][1:])\n",
    "        else:\n",
    "            saver.restore(self.checkpoint_directory + \"-\" + str(global_step))\n",
    "            self.global_step = global_step\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnr = CNN_Regression(device_name=\"gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...  Done 827.2608127593994\n",
      "[EPOCH 1 / STEP 1]\n",
      "Train loss : 108.88072858471423\n",
      "Val   loss : 0.024247907\n",
      "Test  loss : 0.0240\n",
      "\n",
      "[EPOCH 2 / STEP 2]\n",
      "Train loss : 82.50371589511633\n",
      "Val   loss : 0.023112873\n",
      "Test  loss : 0.0230\n",
      "\n",
      "[EPOCH 3 / STEP 3]\n",
      "Train loss : 68.39071556646377\n",
      "Val   loss : 0.023961807\n",
      "Test  loss : 0.0240\n",
      "\n",
      "[EPOCH 4 / STEP 4]\n",
      "Train loss : 56.8140720189549\n",
      "Val   loss : 0.025315262\n",
      "Test  loss : 0.0249\n",
      "\n",
      "[EPOCH 5 / STEP 5]\n",
      "Train loss : 52.97851409902796\n",
      "Val   loss : 0.025482649\n",
      "Test  loss : 0.0255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnnr.fit(X_train, y_train, X_val, y_val, X_test, y_test, 5, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic",
   "language": "python",
   "name": "classic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
