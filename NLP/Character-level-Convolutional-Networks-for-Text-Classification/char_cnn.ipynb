{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir, remove\n",
    "from os.path import isfile, join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/aksdmj/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide filenames by its sentiment\n",
    "# error-avoiding method\n",
    "documents = defaultdict(list)\n",
    "for i in mr.fileids():\n",
    "    documents[i.split('/')[0]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate maximum length of text\n",
    "lens = [len(mr.raw(i)) for i  in mr.fileids()]\n",
    "max_num_characters = max(lens)\n",
    "max_num_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique words used in all texts\n",
    "# 70 non sapce characted is used in original paper.\n",
    "unique_chars = len(set(mr.raw()))\n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(mr.raw())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dict_inv = {v:i for i,v in enumerate(chars)}\n",
    "char_dict_inv['!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum character counts\n",
    "l0 = 1014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_or_padding(chars):\n",
    "    if len(chars) > l0:\n",
    "        # from back\n",
    "        # return chars[-l0:]\n",
    "        # from front\n",
    "        return chars[:l0]\n",
    "    else:\n",
    "        return chars\n",
    "def chars2indexs(chars):\n",
    "    return np.array([char_dict_inv[char] for char in chars])\n",
    "\n",
    "def one_hot(indexs):\n",
    "    temp = np.zeros([l0, unique_chars], dtype=\"float64\")\n",
    "    for i, idx in enumerate(indexs):\n",
    "        if(idx==-1):\n",
    "            continue\n",
    "        temp[i][idx] = 1.0\n",
    "    return temp\n",
    "\n",
    "def preprocess(document):\n",
    "    indexs = chars2indexs(cut_or_padding(mr.raw(document)))\n",
    "    temp = np.concatenate([-1*np.ones([l0 - indexs.shape[0]], dtype=\"int64\"), indexs])\n",
    "    return one_hot(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "y = list()\n",
    "\n",
    "# 0 label for negative , 1 for positive\n",
    "for i in documents['neg']:\n",
    "    tx = preprocess(i)\n",
    "    X.append(tx)\n",
    "    y.append(0)\n",
    "    \n",
    "for i in documents['pos']:\n",
    "    tx = preprocess(i)\n",
    "    X.append(tx)\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation-test split with the ratio of (0.72, 0.18, 0.1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "data_val = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(len(y_val))\n",
    "data_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character_CNN_Sentence(tf.keras.Model):\n",
    "    \"\"\" Classifier for Movie Review dataset\n",
    "    Args:\n",
    "        num_words: uniwue charaters in dataset. \n",
    "        in_dim: dimension of input array, which is l0 in paper.\n",
    "        out_dim: softmax output dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_chars=unique_chars,\n",
    "                 in_dim= l0,\n",
    "                 out_dim=2,\n",
    "                 learning_rate=0.001,\n",
    "                 checkpoint_directory=\"checkpoints/\",\n",
    "                 device_name=\"cpu:0\"):\n",
    "        super(Character_CNN_Sentence, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.num_chars = num_chars\n",
    "        self.out_dim = out_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.device_name = device_name\n",
    "        \n",
    "        self.conv11 = tf.layers.Conv1D(filters=1024, kernel_size=7, padding=\"valid\")\n",
    "        self.conv12 = tf.layers.Conv1D(filters=1024, kernel_size=7, padding=\"valid\")\n",
    "        self.conv13 = tf.layers.Conv1D(filters=1024, kernel_size=3, padding=\"valid\")\n",
    "        self.conv14 = tf.layers.Conv1D(filters=1024, kernel_size=3, padding=\"valid\")\n",
    "        self.conv15 = tf.layers.Conv1D(filters=1024, kernel_size=3, padding=\"valid\")\n",
    "        self.conv16 = tf.layers.Conv1D(filters=1024, kernel_size=3, padding=\"valid\")\n",
    "\n",
    "        self.maxpool1 = tf.layers.MaxPooling1D(pool_size=3, strides=3)\n",
    "        self.maxpool2 = tf.layers.MaxPooling1D(pool_size=3, strides=3)\n",
    "        self.maxpool3 = tf.layers.MaxPooling1D(pool_size=3, strides=3)\n",
    "\n",
    "        self.flatten = tf.layers.Flatten()\n",
    "        self.dropout = tf.layers.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = tf.layers.Dense(2048)        \n",
    "        self.fc2 = tf.layers.Dense(2048)        \n",
    "        self.out = tf.layers.Dense(self.out_dim)        \n",
    "        \n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # global step\n",
    "        self.global_step = 0\n",
    "        \n",
    "        # verbose logging\n",
    "        self.epoch_loss = 0\n",
    "        \n",
    "        # plotting\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_accuracies = []\n",
    "        \n",
    "\n",
    "    def predict(self, X, training):\n",
    "                \n",
    "        x = self.conv11(X)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        x = self.conv16(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        pred = self.out(x)\n",
    "        return pred\n",
    "\n",
    "    def loss(self, X, y, training):\n",
    "        prediction = self.predict(X, training)\n",
    "        loss_val = tf.losses.sparse_softmax_cross_entropy(y, prediction)\n",
    "        # add this for normalization... check otherwise easily overfitted.\n",
    "        loss_val += tf.nn.l2_loss(self.out.weights[0])\n",
    "        self.epoch_loss += loss_val.numpy()\n",
    "        \n",
    "        return loss_val\n",
    "\n",
    "    def grad(self, X, y, training):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss_val = self.loss(X, y, training)\n",
    "        return tape.gradient(loss_val, self.variables) \n",
    "        \n",
    "    def fit(self, train_data, validation_data, test_data, epochs=1, verbose=1, \n",
    "            batch_size=50, saving=False, early_stopping=0):\n",
    "    \n",
    "        train_data_batch = train_data.shuffle(100).batch(batch_size)\n",
    "        \n",
    "        with tf.device(self.device_name):\n",
    "            for i in range(epochs):\n",
    "                self.epoch_loss = 0\n",
    "                for X, y in tfe.Iterator(train_data_batch):\n",
    "                    grads = self.grad(X, y, True)\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                self.global_step += 1\n",
    "                \n",
    "                self.train_losses.append(self.epoch_loss)\n",
    "                \n",
    "                for X, y in tfe.Iterator(validation_data):\n",
    "                    self.val_losses.append(self.loss(X, y, False).numpy())\n",
    "                    \n",
    "                    \n",
    "                accuracy = tfe.metrics.Accuracy('train_acc')\n",
    "\n",
    "                for X, y in tfe.Iterator(test_data):\n",
    "                    logits = self.predict(X=X, training=False)\n",
    "                    predictions = tf.argmax(logits, axis=1, output_type=\"int32\")\n",
    "                    accuracy(predictions, y)     \n",
    "                    \n",
    "                self.test_accuracies.append(accuracy.result().numpy())\n",
    "\n",
    "                if (i+1)%verbose == 0 :\n",
    "                    print(\"[EPOCH %d / STEP %d]\" % ((i + 1), self.global_step))\n",
    "                    print(\"Train loss : %s\" % self.train_losses[-1])\n",
    "                    print(\"Val   loss : %s\" % self.val_losses[-1])\n",
    "                    print(\"TEST accuracy: %.4f%%\" % (self.test_accuracies[-1] * 100))\n",
    "                    print()\n",
    "                          \n",
    "\n",
    "                accuracy.init_variables()\n",
    "                \n",
    "                if early_stopping:\n",
    "                    ok = False\n",
    "                    if len(self.val_losses) <= early_stopping:\n",
    "                        continue\n",
    "                    for i in range(early_stopping):\n",
    "                        if self.val_losses[-(i+1)] < self.val_losses[-(i+2)]:\n",
    "                            ok = True\n",
    "                            break\n",
    "                    if ok:\n",
    "                        continue\n",
    "                    print(\"early stopping on step %s\" % self.global_step)\n",
    "                    break\n",
    "                                              \n",
    "                    \n",
    "                    \n",
    "    def save(self):\n",
    "        tfe.Saver(self.variables).save(self.checkpoint_directory, global_step=self.global_step)\n",
    "\n",
    "    def load(self, global_step=\"latest\"):\n",
    "        # init\n",
    "        self.predict(tf.convert_to_tensor(np.zeros([1, self.num_words]), dtype=\"float64\"), False)\n",
    "\n",
    "        saver = tfe.Saver(self.variables)\n",
    "        if global_step == \"latest\":\n",
    "            saver.restore(tf.train.latest_checkpoint(self.checkpoint_directory))\n",
    "            self.global_step = int(tf.train.latest_checkpoint(self.checkpoint_directory).split('/')[-1][1:])\n",
    "        else:\n",
    "            saver.restore(self.checkpoint_directory + \"-\" + str(global_step))\n",
    "            self.global_step = global_step\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "charcnn = Character_CNN_Sentence(device_name=\"gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 5 / STEP 5]\n",
      "Train loss : 60.09034442901611\n",
      "Val   loss : 2.0160692\n",
      "TEST accuracy: 47.5000%\n",
      "\n",
      "[EPOCH 10 / STEP 10]\n",
      "Train loss : 53.206207036972046\n",
      "Val   loss : 1.9905567\n",
      "TEST accuracy: 50.5000%\n",
      "\n",
      "[EPOCH 15 / STEP 15]\n",
      "Train loss : 52.241761803627014\n",
      "Val   loss : 2.14337\n",
      "TEST accuracy: 48.5000%\n",
      "\n",
      "[EPOCH 20 / STEP 20]\n",
      "Train loss : 49.60462129116058\n",
      "Val   loss : 2.0144515\n",
      "TEST accuracy: 55.0000%\n",
      "\n",
      "[EPOCH 25 / STEP 25]\n",
      "Train loss : 39.84679687023163\n",
      "Val   loss : 2.1091306\n",
      "TEST accuracy: 55.0000%\n",
      "\n",
      "[EPOCH 30 / STEP 30]\n",
      "Train loss : 32.776662945747375\n",
      "Val   loss : 2.3623846\n",
      "TEST accuracy: 54.5000%\n",
      "\n",
      "[EPOCH 35 / STEP 35]\n",
      "Train loss : 31.832338213920593\n",
      "Val   loss : 2.446187\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 40 / STEP 40]\n",
      "Train loss : 31.103715777397156\n",
      "Val   loss : 2.59159\n",
      "TEST accuracy: 55.0000%\n",
      "\n",
      "[EPOCH 45 / STEP 45]\n",
      "Train loss : 30.30930757522583\n",
      "Val   loss : 2.4889421\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 50 / STEP 50]\n",
      "Train loss : 29.477823615074158\n",
      "Val   loss : 2.484789\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 55 / STEP 55]\n",
      "Train loss : 28.593930304050446\n",
      "Val   loss : 2.4729848\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 60 / STEP 60]\n",
      "Train loss : 27.6601123213768\n",
      "Val   loss : 2.449056\n",
      "TEST accuracy: 58.0000%\n",
      "\n",
      "[EPOCH 65 / STEP 65]\n",
      "Train loss : 26.679286897182465\n",
      "Val   loss : 2.4223425\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 70 / STEP 70]\n",
      "Train loss : 25.65419763326645\n",
      "Val   loss : 2.3948114\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 75 / STEP 75]\n",
      "Train loss : 24.589471518993378\n",
      "Val   loss : 2.3599849\n",
      "TEST accuracy: 57.0000%\n",
      "\n",
      "[EPOCH 80 / STEP 80]\n",
      "Train loss : 23.489312171936035\n",
      "Val   loss : 2.3230972\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 85 / STEP 85]\n",
      "Train loss : 22.358253479003906\n",
      "Val   loss : 2.2843428\n",
      "TEST accuracy: 57.5000%\n",
      "\n",
      "[EPOCH 90 / STEP 90]\n",
      "Train loss : 21.201599836349487\n",
      "Val   loss : 2.2444396\n",
      "TEST accuracy: 58.0000%\n",
      "\n",
      "[EPOCH 95 / STEP 95]\n",
      "Train loss : 20.024956107139587\n",
      "Val   loss : 2.196838\n",
      "TEST accuracy: 58.5000%\n",
      "\n",
      "[EPOCH 100 / STEP 100]\n",
      "Train loss : 18.834486424922943\n",
      "Val   loss : 2.1526623\n",
      "TEST accuracy: 59.0000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "charcnn.fit(data_train, data_val,  data_test, epochs=100, verbose=5, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic",
   "language": "python",
   "name": "classic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
